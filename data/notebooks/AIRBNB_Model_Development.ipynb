{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# \n",
    "# # BW AirBNB\n",
    "# \n",
    "# ## Team Members\n",
    "# - Jason Robinson [Model Prediction - NLP] \n",
    "# - Jimmy Slagle [Architecture]\n",
    "# - Brandon Moore [Data Visualization]\n",
    "# - Peter Geraghty [Hyperparamter Tuning]\n",
    "# \n",
    "# ## Project Description\n",
    "# \n",
    "# This project will involve training an Artifical Neural Network to predict the optimal pricing for specific properties of AirBnB locations within the city of New York. Modeling techniques of use will consist of precision-based distribution and accuratcy\n",
    "# \n",
    "\n",
    "# %%\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pathlib\n",
    "\n",
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "#import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Natural language processing\n",
    "import spacy\n",
    "\n",
    "# Neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "\n",
    "print(f'Geopandas Version: {gpd.__version__}')\n",
    "print(f'Tensorflow Version: {tf.__version__}')\n",
    "print(f'Seaborn Version: {sns.__version__}')\n",
    "print(f'Spacy Version: {spacy.__version__}')\n",
    "print(f'Pandas Version: {pd.__version__}')\n",
    "print(f'Numpy Version: {np.__version__}')\n",
    "\n",
    "# %% [markdown]\n",
    "# \n",
    "# ## Load Data\n",
    "# \n",
    "\n",
    "# %%\n",
    "# Load and pre-process data\n",
    "\n",
    "df1 = pd.read_csv('/Users/jasonrobinson/Downloads/AB_NYC_2019.csv')\n",
    "print(df1.shape)\n",
    "df1.head(2)\n",
    "\n",
    "# %%\n",
    "df = df1.copy()\n",
    "\n",
    "# %%\n",
    "df.info()\n",
    "\n",
    "# %%\n",
    "df.isnull().sum()\n",
    "\n",
    "# %%\n",
    "print(df['neighbourhood_group'].unique())\n",
    "print(df['room_type'].unique())\n",
    "df['neighbourhood'][:10].unique()\n",
    "\n",
    "# %%\n",
    "df['room_type'].unique()\n",
    "\n",
    "# %%\n",
    "df['neighbourhood'][:10].unique()\n",
    "\n",
    "# %%\n",
    "# The labels with the highest correalation will have no impact on predicting price by location.\n",
    "corr = df.corr(method='kendall')\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(corr, annot=True);\n",
    "\n",
    "# %% [markdown]\n",
    "# \n",
    "# ## Data Exploration\n",
    "# \n",
    "\n",
    "# %%\n",
    "# Retrieve a description of statistics.\n",
    "df.describe()\n",
    "\n",
    "# %%\n",
    "# Filter out unnecessary columns\n",
    "df = df.drop(columns=['name','id','host_id','host_name', \n",
    "                      'reviews_per_month', 'last_review'], axis=1)\n",
    "\n",
    "# %%\n",
    "# Visualization of neighbourhood locations.\n",
    "plot_dims=(10,9)\n",
    "plt.figure(figsize=plot_dims)\n",
    "sns.scatterplot(df.longitude, df.latitude, hue=df.neighbourhood_group)\n",
    "plt.ioff()\n",
    "\n",
    "# %% [markdown]\n",
    "# \n",
    "# ## Preprocessing\n",
    "# \n",
    "\n",
    "# %%\n",
    "# Normalize our categorical data to have a mean of 0 and std of 1.\n",
    "df['neighbourhood_group']=pd.factorize(df.neighbourhood_group)[0]\n",
    "df['neighbourhood']=pd.factorize(df.neighbourhood)[0]\n",
    "df['room_type']=pd.factorize(df.room_type)[0]\n",
    "\n",
    "# %%\n",
    "# Normalizing the availability column with a mean of 0 and std of 1.\n",
    "availability=df['availability_365']\n",
    "availability=(availability-availability.mean())/availability.std()\n",
    "\n",
    "# %%\n",
    "# Split dataset get our target column.\n",
    "Y = df['price']\n",
    "X = df.drop(df['price'])\n",
    "# Truncate mismatched datasets.\n",
    "Y = Y.truncate(after=48220,axis=0)\n",
    "\n",
    "X.shape,Y.shape\n",
    "\n",
    "# %%\n",
    "# Splitting longitude data to be equal.\n",
    "max_long = df['longitude'].max()\n",
    "min_long = df['longitude'].min()\n",
    "diff = max_long - min_long\n",
    "diff/100\n",
    "\n",
    "long_boundaries = []\n",
    "for i in np.arange(min_long, max_long, diff):\n",
    "    long_boundaries.append(min_long + i * diff)\n",
    "\n",
    "max_lat = df['latitude'].max()\n",
    "min_lat = df['latitude'].min()\n",
    "d = max_lat - min_lat\n",
    "d/100\n",
    "\n",
    "long_boundaries = []\n",
    "for i in np.arange(min_lat, max_lat, d):\n",
    "    long_boundaries.append(min_lat + i * d)\n",
    "\n",
    "# %%\n",
    "# Create a dense layer by defining a bucketed column.\n",
    "longitude = tf.feature_column.bucketized_column(\n",
    "              tf.feature_column.numeric_column('longitude'), boundaries=long_boundaries)\n",
    "\n",
    "latitude = tf.feature_column.bucketized_column(\n",
    "              tf.feature_column.numeric_column('latitude'), boundaries=long_boundaries)\n",
    "\n",
    "crossed_feature = tf.feature_column.crossed_column([longitude, latitude], hash_bucket_size=50)\n",
    "feature_layer = tf.keras.layers.DenseFeatures(tf.feature_column.indicator_column(crossed_feature))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Modeling Approach #1\n",
    "\n",
    "# %%\n",
    "# Split data into train test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "# %%\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(X_train, X_test)\n",
    "\n",
    "# %%\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train,y_train)\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "# %%\n",
    "# Build a sequential model.\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(64, activation='relu', input_shape=[len(X_train)[0]]),\n",
    "    Dense(32, activation='relu'),\n",
    "#    Dense(1, kerner_initializer='normal')\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "\n",
    "# %%\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "# Incorporate reduce on learning rate to cease after reaching optimal improvement.\n",
    "lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.2)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "callbacks = [lr_reduce, early_stop]\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=callbacks, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# %%\n",
    "model.summary()\n",
    "\n",
    "# %%\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()\n",
    "\n",
    "# %%\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Modeling Approach #2\n",
    "\n",
    "# %%\n",
    "# Load weights into new model.\n",
    "loaded_model.load_weights(\"bnb_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# %%\n",
    "\n",
    "# Evaluate loaded model on test data.\n",
    "loaded_model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['MAE'])\n",
    "score = loaded_model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "\n",
    "# %%\n",
    "def model_2():\n",
    "    model2 = Sequential([\n",
    "      Dense(128, activation='relu', input_shape=[len(X_train)]),\n",
    "      Dense(64, activation='relu'),\n",
    "      Dense(1)\n",
    "    ])\n",
    "    \n",
    "    return model2  \n",
    "\n",
    "# %%\n",
    "model2 = model_2()\n",
    "\n",
    "# %%\n",
    "learning_rate=0.001\n",
    "opt = SGD(lr=learning_rate)\n",
    "model2.compile(optimizer=opt, \n",
    "              loss='mean_squared_error',\n",
    "              metrics=['MAE'])\n",
    "\n",
    "# %%\n",
    "%%time\n",
    "# Fit our model and demonstrate time.\n",
    "history2 = model2.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    verbose=0,\n",
    "    validation_split = 0.2)\n",
    "\n",
    "# %%\n",
    "# Visualize the training progress using the information from the history object.\n",
    "def plot_loss(history2):\n",
    "  plt.plot(history.history2['loss'], label='loss')\n",
    "  plt.plot(history.history2['val_loss'], label='val_loss')\n",
    "  plt.ylim([0, 10])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [price]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "plot_loss(history2)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Save Final Model\n",
    "\n",
    "# %%\n",
    "#pip install -q pyyaml h5py\n",
    "\n",
    "# %%\n",
    "import h5py\n",
    "model.save('bnb_model.h5')\n",
    "\n",
    "# %%\n",
    "# Serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"bnb_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# Serialize weights to HDF5\n",
    "model.save_weights(\"bnb_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# %%\n",
    "# Load json and create model\n",
    "json_file = open('bnb_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
